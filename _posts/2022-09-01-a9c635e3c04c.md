---
title: 【Python】喝爬蟲湯前「請求」先備料 — Request
author: Martin
date: 2022-09-01T08:34:41.725+0000
last_modified_at: 2022-09-01T08:34:41.725+0000
categories: 
tags: []
description: 上一篇文講到爬蟲會使用到Beautiful soup 解析HTML 或是XML 文件，進而從文件中拿到需要的資料，那文件從哪來勒？這篇文就是在爬蟲前先把被爬的素材準備好
render_with_liquid: false
---

### 【Python】喝爬蟲湯前「請求」先備料 — Request

上一篇文講到爬蟲會使用到Beautiful soup 解析HTML 或是XML 文件，進而從文件中拿到需要的資料，那文件從哪來勒？這篇文就是在爬蟲前先把被爬的素材準備好

上文件～～（不得不說，Request 的圖示畫的真的很好看


![](/assets/a9c635e3c04c/1*tbngDyXetl-ssbICTo0SNA.png)



[![]()](https://requests.readthedocs.io/en/latest/)


其實Request 這個套件就是在「模擬一般使用者用瀏覽器對網頁發出請求」的這個動作，因此平常在瀏覽器上做的行為都可以透過Request 來做，像是瀏覽網頁\(GET\)、送出表單\(POST\)、更新部落格資料\(PUT\)……。
#### 瀏覽網頁\(GET\)

如果只是單純要取得某個網頁資料，可以用以下方法簡單做到：
```
# 引入 requests 模組
import requests
```

當我們透過 `request.get` 這個方法取得網頁資料，並儲存在 `response` 這個變數裡時， `response` 內建一些方法讓我們可以確認取得的資料。首先當然要確保被請求的伺服器所傳回來的回應是否正常，這時就會需要看 `response` 的狀態碼（ [狀態碼代表意義](https://www.newscan.com.tw/all-seo/http-status-codes.htm) ）
```
print(response.status_code)
# 200
print(response.raise_for_status())
# None 
## All is well.XD
```

確認 `response` 回應沒問題後，就可以放心使用取回的資料：
```
response.text 
# 取得透過unicode解析後的回應，預設為'utf-8'
# 可以透過respnse.encoding = 'ISO-8859-1' 來設定
response.content
# 取得位元模式的回應
```

在許多網站中，都會將搜尋的關鍵字放進網址裡，這種情況的請求也可以透過Request 來模擬，雖然也可以先處理好請求的URL 後再直接指定給Request，但如果參數裡有非英數的情況時，就會牽扯到編碼的問題，因此還是透過Request 來做比較一勞永逸：
```
# 搜尋參數
search_params = {'key1': 'value1', 'key2': 'value2'}
r = requests.get('http://httpbin.org/get', params = search_params)
```

如果爬蟲的次數過於頻繁，就會被許多有設定阻止爬蟲的網站攔截，此時可以特過自訂表頭\(header\) 的方式盡量避免被攔截：
```
# 自訂表頭
my_headers = { "Accept-Encoding": "gzip, deflate, br" }

# 將自訂表頭加入 GET 請求中
r = requests.get('http://httpbin.org/get', headers = my_headers)
```

如果短時間內使用相同的表頭做請求，依然容易被判斷為爬蟲而被阻擋，此時就可以使用表頭中的「使用者代理\(User\-Agent\)」欄位告訴網站該請求是透過哪個瀏覽器發出的
```
# 自訂表頭
my_headers = { 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64)' }
r = requests.get('http://httpbin.org/get', headers = my_headers)
```

如果都使用相同的使用者代理，那有等於沒有，因此為了避免被偵測到是網頁爬蟲，這時候就可以隨機的替換使用者代理：
```
# 使用fake_useragent套件，在每一次的請求(Request)前，隨機產生一個使用者代理
from fake_useragent import UserAgent
user_agent = UserAgent()
```

如果回應的網站資料中含有cookie，Request 也可以輕鬆取出，或是要在請求中帶上cookie，也辦得到：
```
r = requests.get("http://my.server.com/has/cookies")

# 取出 cookie
print(r.cookies['my_cookie_name'])
---
my_cookies = dict(my_cookie_name='G. T. Wang')
```

除了上述講到的 `params` 、 `cookie` 、 `header` 以外，還可以在請求中帶入 `timeout` 、 `verify` 等其他參數，更詳細的當然就參考文件拉～～～

參考資料：


[![](https://blog.gtwang.org/wp-content/uploads/2018/01/arrows-python-logo-20180126-1.jpg)](https://blog.gtwang.org/programming/python-requests-module-tutorial/)



[![]()](https://blog.csdn.net/xieminglu/article/details/109270305)



[![](https://1.bp.blogspot.com/-M9IaX4jCXNg/X2bqxx3crsI/AAAAAAAAED8/5nDHp6D9l_gw1Y6GhtjN3_ilUnX5H08iACLcBGAsYHQ/w1200-h630-p-k-no-nu/7_tips_to_avoid_getting_blocked_while_scraping.jpg)](https://www.learncodewithmike.com/2020/09/7-tips-to-avoid-getting-blocked-while-scraping.html)





